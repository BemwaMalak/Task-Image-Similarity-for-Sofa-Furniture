{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, Any, Optional, List\n",
    "\n",
    "\n",
    "class ImagePreprocessor(ABC):\n",
    "    \"\"\"Abstract base class for image preprocessing algorithms.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess(self, image_path: str, save_path: Optional[str] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess the input image.\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to the input image\n",
    "            save_path (str, optional): Path to save the processed image\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Processed image\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class SofaSegmenter(ImagePreprocessor):\n",
    "    \"\"\"Sofa segmentation and background removal using GrabCut algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, padding: int = 10, max_size: int = 800, iterations: int = 1):\n",
    "        \"\"\"\n",
    "        Initialize the sofa segmenter.\n",
    "        \n",
    "        Args:\n",
    "            padding (int): Padding to add around the segmented sofa\n",
    "            max_size (int): Maximum dimension size for resizing while maintaining aspect ratio\n",
    "            iterations (int): Number of GrabCut iterations\n",
    "        \"\"\"\n",
    "        if padding < 0 or max_size <= 0 or iterations <= 0:\n",
    "            raise ValueError(\"Invalid parameters: padding must be >= 0, max_size and iterations must be > 0\")\n",
    "        \n",
    "        self.padding = padding\n",
    "        self.max_size = max_size\n",
    "        self.iterations = iterations\n",
    "        \n",
    "    def _resize_image(self, image: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Resize image while maintaining aspect ratio if it exceeds max_size.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, float]: (Resized image, scale factor)\n",
    "        \"\"\"\n",
    "        height, width = image.shape[:2]\n",
    "        max_dim = max(height, width)\n",
    "        \n",
    "        if max_dim > self.max_size:\n",
    "            scale = self.max_size / max_dim\n",
    "            new_width = int(width * scale)\n",
    "            new_height = int(height * scale)\n",
    "            resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "            return resized, scale\n",
    "        return image, 1.0\n",
    "\n",
    "    def _create_initial_mask(self, height: int, width: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create initial mask for GrabCut with sofa-specific regions.\n",
    "        \n",
    "        Args:\n",
    "            height (int): Image height\n",
    "            width (int): Image width\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Initialized mask with background/foreground regions\n",
    "        \"\"\"\n",
    "        # Initialize as probable background\n",
    "        mask = np.ones((height, width), np.uint8) * cv2.GC_PR_BGD\n",
    "        \n",
    "        # Border parameters\n",
    "        border = int(min(height, width) * 0.05)\n",
    "        \n",
    "        # sofa typically occupies the central portion of the image\n",
    "        sofa_regions = {\n",
    "            'outer': {'y': (0.2, 0.8), 'x': (0.1, 0.9), 'value': cv2.GC_PR_FGD},\n",
    "            'inner': {'y': (0.3, 0.7), 'x': (0.2, 0.8), 'value': cv2.GC_FGD}\n",
    "        }\n",
    "        \n",
    "        # Mark borders as definite background\n",
    "        mask[:border, :] = cv2.GC_BGD\n",
    "        mask[-border:, :] = cv2.GC_BGD\n",
    "        mask[:, :border] = cv2.GC_BGD\n",
    "        mask[:, -border:] = cv2.GC_BGD\n",
    "        \n",
    "        # Mark sofa regions\n",
    "        for region in sofa_regions.values():\n",
    "            y_start = int(height * region['y'][0])\n",
    "            y_end = int(height * region['y'][1])\n",
    "            x_start = int(width * region['x'][0])\n",
    "            x_end = int(width * region['x'][1])\n",
    "            mask[y_start:y_end, x_start:x_end] = region['value']\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def _get_bounding_box(self, mask: np.ndarray) -> Tuple[int, int, int, int]:\n",
    "        \"\"\"\n",
    "        Get bounding box coordinates from the largest contour in the mask.\n",
    "        \n",
    "        Args:\n",
    "            mask: Binary mask\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[int, int, int, int]: (x, y, width, height) of bounding box\n",
    "        \"\"\"\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            return cv2.boundingRect(largest_contour)\n",
    "        return (0, 0, mask.shape[1], mask.shape[0])\n",
    "\n",
    "    def _segment_sofa(self, image: np.ndarray) -> Tuple[np.ndarray, Tuple[int, int, int, int]]:\n",
    "        \"\"\"\n",
    "        Segment sofa from image using GrabCut algorithm.\n",
    "        \n",
    "        Args:\n",
    "            image: Input BGR image\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, Tuple[int, int, int, int]]: (Segmented image, bounding box)\n",
    "        \"\"\"\n",
    "        height, width = image.shape[:2]\n",
    "        mask = self._create_initial_mask(height, width)\n",
    "        \n",
    "        background_model = np.zeros((1, 65), np.float64)\n",
    "        foreground_model = np.zeros((1, 65), np.float64)\n",
    "        \n",
    "        # Perform GrabCut segmentation\n",
    "        cv2.grabCut(image, mask, None, background_model, foreground_model, \n",
    "                   self.iterations, cv2.GC_INIT_WITH_MASK)\n",
    "        \n",
    "        # Create binary mask and apply it\n",
    "        binary_mask = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "        segmented_image = cv2.bitwise_and(image, image, mask=binary_mask)\n",
    "        \n",
    "        return segmented_image, self._get_bounding_box(binary_mask)\n",
    "\n",
    "    def preprocess(self, image_path: str, save_path: Optional[str] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess sofa image using GrabCut-based segmentation.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            save_path: Optional path to save the processed image\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Processed image with background removed and cropped\n",
    "        \"\"\"\n",
    "        # Load and validate image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image from {image_path}\")\n",
    "        \n",
    "        # Process image at reduced size for efficiency\n",
    "        resized_image, scale = self._resize_image(image)\n",
    "        segmented_image, (x, y, w, h) = self._segment_sofa(resized_image)\n",
    "        \n",
    "        # Scale coordinates back to original size if needed\n",
    "        if scale != 1.0:\n",
    "            x, y, w, h = [int(val / scale) for val in (x, y, w, h)]\n",
    "            segmented_image = cv2.resize(segmented_image, (image.shape[1], image.shape[0]), \n",
    "                                       interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Add padding and ensure coordinates are within image bounds\n",
    "        x = max(0, x - self.padding)\n",
    "        y = max(0, y - self.padding)\n",
    "        w = min(image.shape[1] - x, w + 2 * self.padding)\n",
    "        h = min(image.shape[0] - y, h + 2 * self.padding)\n",
    "        \n",
    "        # Crop to sofa region\n",
    "        result = segmented_image[y:y+h, x:x+w]\n",
    "        \n",
    "        if save_path:\n",
    "            cv2.imwrite(save_path, result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(ABC):\n",
    "    \"\"\"Abstract base class for feature extractors.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract_features(self, image: np.ndarray) -> Tuple[list, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Extract features from an image.\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): Input image as a numpy array (BGR format)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (keypoints, descriptors)\n",
    "                - keypoints: List of keypoint objects\n",
    "                - descriptors: numpy array of descriptors or None if no features found\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute_similarity(self, desc1: np.ndarray, desc2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity between two sets of descriptors.\n",
    "        \n",
    "        Args:\n",
    "            desc1 (np.ndarray): First set of descriptors\n",
    "            desc2 (np.ndarray): Second set of descriptors\n",
    "            \n",
    "        Returns:\n",
    "            float: Similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(img1: np.ndarray, img2: np.ndarray, similarity_score: Optional[float] = None) -> None:\n",
    "    \"\"\"\n",
    "    Display two images side by side.\n",
    "    \n",
    "    Args:\n",
    "        img1 (np.ndarray): First image as numpy array (BGR format)\n",
    "        img2 (np.ndarray): Second image as numpy array (BGR format)\n",
    "        similarity_score (float, optional): Similarity score to display\n",
    "    \"\"\"\n",
    "    # Convert from BGR to RGB\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    \n",
    "    # Display images\n",
    "    ax1.imshow(img1_rgb)\n",
    "    ax1.set_title('Query Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(img2_rgb)\n",
    "    ax2.set_title('Most Similar Image')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    if similarity_score is not None:\n",
    "        plt.suptitle(f'Similarity Score: {similarity_score:.2f}', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_sofa(query_image: np.ndarray, database_dir: str, feature_extractor: FeatureExtractor) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Find the most similar sofa image in the database using feature_extractor.\n",
    "    \n",
    "    Args:\n",
    "        query_image (np.ndarray): Query image as numpy array (BGR format)\n",
    "        database_dir (str): Directory containing database images\n",
    "        feature_extractor (FeatureExtractor): Feature extractor object\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (most_similar_image, similarity_score, image_path)\n",
    "            - most_similar_image: numpy array of the most similar image\n",
    "            - similarity_score: float indicating similarity (0-1)\n",
    "    \"\"\"\n",
    "    # Extract features from query image\n",
    "    query_features = feature_extractor.extract_features(query_image)\n",
    "    \n",
    "    best_score = -1\n",
    "    best_match = None\n",
    "    \n",
    "    # Compare with all images in database\n",
    "    for img_path in Path(database_dir).glob('*.jpg'):\n",
    "        # Load database image\n",
    "        db_img = cv2.imread(str(img_path))\n",
    "        if db_img is None:\n",
    "            print(f\"Warning: Could not read image at {img_path}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Extract features and compute similarity\n",
    "        db_features = feature_extractor.extract_features(db_img)\n",
    "        score = feature_extractor.compute_similarity(query_features, db_features)\n",
    "        \n",
    "        # Update best match if current score is higher\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = db_img\n",
    "                \n",
    "    if best_match is None:\n",
    "        raise ValueError(\"No valid matches found in the database\")\n",
    "    \n",
    "    return best_match, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Color Feature Extraction\n",
    "\n",
    "### HSV Color Space\n",
    "- The image is converted from BGR to HSV (Hue, Saturation, Value) color space\n",
    "- HSV is chosen because it better represents how humans perceive color:\n",
    "  - Hue: The actual color (0-180 degrees)\n",
    "  - Saturation: Color intensity (0-255)\n",
    "  - Value: Brightness (0-255)\n",
    "\n",
    "### Color Histograms\n",
    "- Separate histograms are computed for each HSV channel using 32 bins\n",
    "- Each histogram is normalized to [0,1] range for consistent comparison\n",
    "- The three histograms are concatenated to form a single color feature vector\n",
    "\n",
    "## 2. Texture Feature Extraction\n",
    "\n",
    "### GLCM (Gray-Level Co-occurrence Matrix)\n",
    "- The image is converted to grayscale and resized to 64x64 for consistency\n",
    "- Gray levels are reduced to 8 for computational efficiency\n",
    "- GLCM captures spatial relationships between pixels by counting how often pairs of pixels with specific values occur in a specific spatial relationship\n",
    "\n",
    "### Texture Features Extracted from GLCM\n",
    "Four statistical measures are computed:\n",
    "1. **Contrast**: Measures local variations in the GLCM\n",
    "2. **Homogeneity**: Measures closeness of element distribution in GLCM\n",
    "3. **Energy**: Sum of squared elements in GLCM\n",
    "4. **Correlation**: Measures linear dependency of gray levels\n",
    "\n",
    "## 3. Similarity Computation\n",
    "\n",
    "The similarity between two images is calculated using a weighted combination:\n",
    "\n",
    "### Color Similarity (70% weight)\n",
    "- Uses correlation between color histograms\n",
    "- Correlation measures the statistical relationship between histograms\n",
    "\n",
    "### Texture Similarity (30% weight)\n",
    "- Uses cosine similarity between texture feature vectors\n",
    "- Cosine similarity measures the angle between feature vectors\n",
    "\n",
    "### Final Score\n",
    "- Combined score = 0.7 * color_similarity + 0.3 * texture_similarity\n",
    "- Score is clamped to [0,1] range\n",
    "\n",
    "The weights (0.7 for color, 0.3 for texture) can be adjusted based on specific requirements.\n",
    "\n",
    "I think this approach is a good option for feature extraction and similarity computation rather than SIFT because it takes color into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorHistogramExtractor(FeatureExtractor):\n",
    "    \"\"\"Color Histogram and Texture feature extractor implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, hist_bins=32):\n",
    "        \"\"\"\n",
    "        Initialize the Color Histogram extractor.\n",
    "        \n",
    "        Args:\n",
    "            hist_bins (int): Number of bins for the color histogram\n",
    "        \"\"\"\n",
    "        self.hist_bins = hist_bins\n",
    "        \n",
    "    def extract_features(self, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extract color histogram and texture features from an image.\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): Input image as a numpy array (BGR format)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (color_hist, texture_features)\n",
    "                - color_hist: Combined color histogram for all channels\n",
    "                - texture_features: Texture features from gray-scale image\n",
    "        \"\"\"\n",
    "        if image is None or image.size == 0:\n",
    "            raise ValueError(\"Invalid input image\")\n",
    "            \n",
    "        # Convert BGR to HSV color space\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate color histogram for each channel\n",
    "        hist_h = cv2.calcHist([hsv], [0], None, [self.hist_bins], [0, 180])\n",
    "        hist_s = cv2.calcHist([hsv], [1], None, [self.hist_bins], [0, 256])\n",
    "        hist_v = cv2.calcHist([hsv], [2], None, [self.hist_bins], [0, 256])\n",
    "        \n",
    "        # Normalize histograms\n",
    "        cv2.normalize(hist_h, hist_h, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "        cv2.normalize(hist_s, hist_s, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "        cv2.normalize(hist_v, hist_v, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Combine histograms\n",
    "        color_features = np.concatenate([hist_h, hist_s, hist_v])\n",
    "        \n",
    "        # Calculate texture features using gray-scale image\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Calculate GLCM (Gray-Level Co-Occurrence Matrix) features\n",
    "        glcm = self._calculate_glcm(gray)\n",
    "        texture_features = self._extract_glcm_features(glcm)\n",
    "        \n",
    "        return color_features, texture_features\n",
    "    \n",
    "    def compute_similarity(self, features1: Tuple[np.ndarray, np.ndarray], \n",
    "                         features2: Tuple[np.ndarray, np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity between two sets of features.\n",
    "        \n",
    "        Args:\n",
    "            features1: Tuple of (color_hist1, texture_features1)\n",
    "            features2: Tuple of (color_hist2, texture_features2)\n",
    "            \n",
    "        Returns:\n",
    "            float: Similarity score between 0 and 1\n",
    "        \"\"\"\n",
    "        color_hist1, texture_features1 = features1\n",
    "        color_hist2, texture_features2 = features2\n",
    "        \n",
    "        # Compare color histograms using correlation\n",
    "        color_similarity = cv2.compareHist(color_hist1, color_hist2, cv2.HISTCMP_CORREL)\n",
    "        \n",
    "        # Compare texture features using cosine similarity\n",
    "        texture_similarity = self._cosine_similarity(texture_features1, texture_features2)\n",
    "        \n",
    "        # Combine similarities\n",
    "        combined_similarity = 0.7 * color_similarity + 0.3 * texture_similarity\n",
    "        \n",
    "        return max(0.0, min(1.0, combined_similarity))\n",
    "    \n",
    "    def _calculate_glcm(self, gray_image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate Gray-Level Co-Occurrence Matrix.\"\"\"\n",
    "        gray_image = cv2.resize(gray_image, (64, 64))\n",
    "        levels = 8\n",
    "        gray_image = ((gray_image / 256) * levels).astype(np.uint8)\n",
    "        \n",
    "        glcm = np.zeros((levels, levels))\n",
    "        h, w = gray_image.shape\n",
    "        \n",
    "        for i in range(h-1):\n",
    "            for j in range(w-1):\n",
    "                current = gray_image[i, j]\n",
    "                right = gray_image[i, j+1]\n",
    "                glcm[current, right] += 1\n",
    "                \n",
    "        # Normalize GLCM\n",
    "        glcm = glcm / glcm.sum()\n",
    "        return glcm\n",
    "    \n",
    "    def _extract_glcm_features(self, glcm: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Extract features from GLCM.\"\"\"\n",
    "        contrast = np.sum(np.square(np.arange(glcm.shape[0])) * glcm)\n",
    "        homogeneity = np.sum(glcm / (1 + np.square(np.arange(glcm.shape[0]))))\n",
    "        energy = np.sum(np.square(glcm))\n",
    "        correlation = np.sum(glcm * np.outer(np.arange(glcm.shape[0]), np.arange(glcm.shape[0])))\n",
    "        \n",
    "        return np.array([contrast, homogeneity, energy, correlation])\n",
    "    \n",
    "    def _cosine_similarity(self, v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/sofas/processed\"\n",
    "query_image_path = \"../data/sofas/test/image_1.jpg\"\n",
    "\n",
    "query_img = SofaSegmenter(\n",
    "    padding=20,\n",
    "    max_size=800\n",
    ").preprocess(query_image_path)\n",
    "feature_extractor = ColorHistogramExtractor()\n",
    "\n",
    "# Find most similar image\n",
    "similar_image, similarity_score = find_most_similar_sofa(query_img, data_dir, feature_extractor)\n",
    "print(f\"Similarity score: {similarity_score:.2f}\")\n",
    "\n",
    "# Visualize matches\n",
    "visualize_images(query_img, similar_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
